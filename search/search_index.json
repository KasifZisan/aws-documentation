{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Amazon Web Services (AWS)","text":"<p>Fundamental AWS servies - Compute, Storage, Database and Network</p>"},{"location":"#compute","title":"Compute","text":"<p>In the context of AWS (Amazon Web Services), compute refers to the resources and services that enable you to run applications, process data, and execute tasks. Essentially, compute refers to the virtual machines (VMs), containers, or serverless functions that provide the processing power for your workloads.</p> <p>Amazon EC2 - Amazon Elastic Compute Cloud</p>"},{"location":"#storage","title":"Storage","text":"<p>In the context of AWS (Amazon Web Services), storage refers to the services and resources that allow you to store and manage data in the cloud. AWS offers a variety of storage options, each optimized for different types of data, use cases, and performance needs, such as storing files, databases, backups, or archives.</p> <p>Amazon S3 - Amazon Simple Storage Service</p>"},{"location":"ec2/","title":"Amazon Elastic Compute Cloud (EC2)","text":"<p>It allows us to deploy virtual servers within the AWS environment.</p>"},{"location":"ec2/#ec2-components","title":"EC2 Components","text":"<ul> <li>Amazon Machine Images (AMI): Pre configured EC2 instances that allow to quickly launch a new EC2 instance based on the config defined in the AMI. An AMI can help in auto scaling where multiple instances of the AMI can be quickly created.</li> <li>Instance Types: Defines the size of the instance based on different parameters - vCPUs, Architecture, Memory, Storage, Storage type, Network performance</li> <li>Instance purchasing options: On demand instances [can be launched anytime, can be used for as long as needed, flat rate based on instance types, typically used for short-term workloads], Spot Instances [on demand instances that are not taken currently, these are based on supply and demand], Reserved Instances [purchase a discounted on-demand instance for a set period], On-Demand Capacity Reservations [reserve capacity based on attribute]</li> <li>Tenancy: Shared Tenancy [by default, instances run on available hosts with selected resources], Dedicated Tenancy [runs on Dedicated Instances and Dedicated Hosts]</li> <li>Uesr Data: Allows to enter commands that will run during the first boot cycle of that instance. </li> <li>Storage Options: Persistent Storage [EBS Volumes, also can be taken a snapshot of and store in an Amazon S3 Bucket], Ephemeral Storage [Created by EC2 instances using local storage]</li> <li>Security: Security group is an instance level firewall that protects traffic from ingress and egress requests. Restrict communications using source ports and protocols. Key-Pair Login: Private key and Public key system</li> </ul>"},{"location":"ec2/#virtual-private-cloud-vpc","title":"Virtual Private Cloud (VPC)","text":"<p>A VPC (Virtual Private Cloud) is a logically isolated section of a cloud provider\u2019s network.</p> <ul> <li>Isolation: Resources inside a VPC are isolated.</li> <li>Customization: We can customize the IP address range, subnets, routing and security settings like network access control list (ACL) and security groups.</li> <li>Security: Firewalls, Private Subnets and VPNs</li> <li>Network Config: We can configure routing tables, internet gateways and even peering connections.</li> </ul>"},{"location":"ec2/#vpc-endpoint","title":"VPC Endpoint","text":"<p>A VPC Endpoint allows you to privately connect your VPC to support AWS services (like S3, DynamoDB, etc.) without requiring an internet connection, NAT gateway, or VPN connection.</p>"},{"location":"ec2/#cidr-classless-inter-domain-routing","title":"CIDR (Classless Inter-Domain Routing)","text":"<p>It is a method for assigning IP addresses and IP routing that allows more efficient and flexible allocation of IP addresses. It provides a way to define network addresses with variable-length subnet masks, enabling networks of different sizes to be created.</p> <p>CIDR notations are typically as - <code>IP address/prefix length</code></p>"},{"location":"ec2/#subnet","title":"Subnet","text":"<p>A subnet (short for subnetwork) is a logical division of an IP network into smaller, manageable parts. It allows you to break a larger network into smaller segments to improve performance, security, and manageability.</p>"},{"location":"ec2/#subnet-mask","title":"Subnet Mask","text":"<p>A subnet mask is used to separate the network portion of an IP address from the host portion. In CIDR notation, the subnet mask is directly related to the prefix length. For <code>192.168.0.0/24</code>, the subnet mask is written as <code>255.255.255.0</code>.</p>"},{"location":"ec2/#ipv4-and-ipv6-cidr","title":"IPV4 and IPV6 CIDR","text":"<p>IPv4 addresses are 32 bits long, written in four octets (e.g., 192.168.0.1), which gives around 4.3 billion unique addresses.</p> <p>For example - <code>192.168.0.0/24</code>, This represents all IP addresses from <code>192.168.0.0</code> to <code>192.168.0.255</code></p> <p>IPv6 addresses are 128 bits long, written in hexadecimal and separated by colons (e.g., 2001:0db8:85a3::8a2e:0370:7334). IPv6 can support a vastly larger number of addresses\u2014about 340 undecillion addresses.</p>"},{"location":"ec2/#routing-table","title":"Routing Table","text":"<p>In a routing table, the destination and target (sometimes called \"next hop\") columns are key components that determine how network traffic is routed. </p> <ul> <li>Destination Column: The destination column in the routing table specifies the destination network or destination IP address that the packet is trying to reach.</li> <li>Target Column: The target (or \"next hop\") column specifies where the packet should go next to reach the destination.</li> </ul> <p>The CIDR block must not be the same or larger than a destination CIDR range in a route in any of the VPC route tables.</p>"},{"location":"ec2/#internet-gateway-igw","title":"Internet Gateway (IGW)","text":"<p>IGW is a component that helps VPCs connect with the internet or other external networks. It serves as the bridge between the private network (your VPC) and the public Internet.</p> <ul> <li>Bidirectional Traffic:  The Internet Gateway allows instances within the VPC to send and receive traffic from the Internet. </li> <li>Public and Private IP Handling: For the Internet Gateway to work, instances in the VPC need either: Public IP address, Elastic IP address, NAT Gateways</li> </ul>"},{"location":"ec2/#network-address-translation-nat-gateway","title":"Network Address Translation (NAT) Gateway","text":"<p>A NAT gateway enables instances within a private subnet in a VPC to access the internet for outbound traffic. The NAT gateway is a fully managed service provided by AWS, meaning AWS automatically takes care of scaling, fault tolerance, and maintenance. It can scale automatically to accommodate large amounts of traffic.</p>"},{"location":"ec2/#virtual-private-gateway-vgw","title":"Virtual Private Gateway (VGW)","text":"<p>A Virtual Private Gateway (VGW) enables communication between an on-premises network and a cloud-based Virtual Private Cloud (VPC). </p>"},{"location":"eks/","title":"AWS EKS","text":""},{"location":"eks/#amazon-elastic-kubernetes-service-eks","title":"Amazon Elastic Kubernetes Service (EKS)","text":"<p>Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed Kubernetes service that enables you to run Kubernetes seamlessly in both AWS Cloud and on-premises data centers.</p> <p>First you need to get into the AWS platform and then search EC2 and create a Cluster and Node Group. You can then interact with your EKS cluster by SSH-ing into your EKS cluster. Alternatively, you can also set up an EC2 instance to interact with your EKS cluster.</p> <p>In preparation to manage your EKS Kubernetes cluster, you will need to install several Kubernetes management-related tools and utilities. In this lab step, you will install:</p> <ul> <li>kubectl: the Kubernetes command-line utility which is used for communicating with the Kubernetes Cluster API server</li> <li>awscli: used to query and retrieve your Amazon EKS cluster connection details, written into the <code>~/.kube/config</code> file </li> </ul>"},{"location":"eks/#installation-and-setup-steps","title":"Installation and Setup Steps","text":"<ul> <li>Download the <code>kubectl</code> utility, give it executable permissions, and copy it into a directory that is part of the PATH environment variable:</li> </ul> <pre><code>curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.31.0/2024-09-12/bin/linux/amd64/kubectl\nchmod +x ./kubectl\nsudo cp ./kubectl /usr/local/bin\nexport PATH=/usr/local/bin:$PATH\n</code></pre> <ul> <li>Test the kubectl utility, ensuring that it can be called like so:</li> </ul> <pre><code>kubectl version --client=true\n</code></pre> <ul> <li>Download the AWS CLI utility, give it executable permissions, and copy it into a directory that is part of the PATH environment variable:</li> </ul> <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre> <ul> <li>Test the <code>aws</code> utility -</li> </ul> <pre><code>aws --version\n</code></pre> <ul> <li>Use the <code>aws</code> utility, to retrieve EKS Cluster name:</li> </ul> <pre><code>EKS_CLUSTER_NAME=$(aws eks list-clusters --region us-west-2 --query clusters[0] --output text)\necho $EKS_CLUSTER_NAME\n</code></pre> <ul> <li>Use the <code>aws</code> utility to query and retrieve your Amazon EKS cluster connection details, saving them into the ~/.kube/config file. Enter the following command in the terminal:</li> </ul> <pre><code>aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --region us-west-2 \n</code></pre> <ul> <li>View the EKS Cluster connection details. This confirms that the EKS authentication credentials required to authenticate have been correctly copied into the ~/.kube/config file. Enter the following command in the terminal:</li> </ul> <pre><code>cat ~/.kube/config \n</code></pre>"},{"location":"eks/#deploying-a-cloud-native-application","title":"Deploying a Cloud Native Application","text":"<p>The cloud native application has three parts - Frontend, Backend and Database. Both the Frontend and Backend has particular Service and Deployment manifest for them. The Database has StatefulSet, Service, Persistent Volume (PV) and Persistent Volume Claim (PVC).</p> <ul> <li>StatefulSet - used to deploy and launch 3 x pods containing the MongoDB service configured to listen on port 27017</li> <li>Service - headless, will be created to sit in front of the StatefulSet, creating a stable network name for each of the individual pods as well as for the StatefulSet as a whole</li> <li>Persistent Volume (PV) - x3, 1 for each individual pod in the StatefulSet - MongoDB will be configured to persist all data and configuration into a directory mounted to the persistent volume</li> <li>Persistent Volume Claim (PVC) - x3, 1 for each PV, binds a PV to a Pod within the MongoDB StatefulSet</li> </ul>"},{"location":"eks/#deployment-steps","title":"Deployment Steps","text":"<ul> <li>Begin by creating a namespace,  which will be used to contain all of the cluster resources that will eventually make up the sample cloud native application. In the terminal run the following command:</li> </ul> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: cloudacademy\n  labels:\n    name: cloudacademy\nEOF\n</code></pre> <ul> <li>Configure the <code>cloudacademy</code> namespace to be the default. In the terminal run the following command:</li> </ul> <pre><code>kubectl config set-context --current --namespace cloudacademy\n</code></pre>"},{"location":"eks/#mongodb-deployment","title":"MongoDB Deployment","text":"<ul> <li>Deploy the MongoDB 3 x ReplicaSet</li> <li>Display the available EKS storage classes. In the terminal run the following command:</li> </ul> <pre><code>kubectl get storageclass\n</code></pre> <ul> <li>Create a new Mongo StatefulSet name <code>mongo</code>. Also keep in mind to use the <code>storageclass</code> we got before in the <code>storageClassName</code>. <ul> <li>Examine the MongoDB Pods ordered sequence launch. Run a <code>watch</code> on the pods in the current namespace. Wait until all 3 MongoDB pods (<code>mongo-0</code>, <code>mongo-1</code>, <code>mongo-2</code>) have reached and recorded the Running status. <code>kubectl get pods --watch</code></li> <li>View the pods with the db label in the current namespace. In the terminal run the following command: <code>kubectl get pods -l role=db</code></li> <li>Display the MongoDB Pods, Persistent Volumes and Persistent Volume Claims. <code>kubectl get pod,pv,pvc</code></li> </ul> </li> <li>Create a new Headless Service for Mongo. Examine the newly created <code>mongo</code> Headless Service. <code>kubectl get svc</code></li> <li>Create a temporary network utils pod. Enter into a bash session within it. <code>kubectl run --rm utils -it --image praqma/network-multitool -- bash</code>. Creating a temporary utils pod in the current namespace - ensures that the <code>nslookup</code> queries to be performed next, are done so in the same networking space in which the MongoDB deployment has taken place in.<ul> <li>Within the new utils pod shell, execute the following DNS queries: <code>for i in {0..2}; do nslookup mongo-$i.mongo; done</code>. This confirms that the DNS records have been created successfully and can be resolved within the cluster, 1 per MongoDB pod that exists behind the Headless Service - earlier created. Exit the utils container: <code>exit</code></li> </ul> </li> <li>Confirm that the <code>mongo</code> shell can now also resolve each of the 3 Mongo headless Service assigned DNS names. In the terminal run the following command:</li> </ul> <pre><code>for i in {0..2}; do kubectl exec -it mongo-0 -- mongo mongo-$i.mongo --eval \"print('mongo-$i.mongo SUCCEEDED\\n')\"; done\n</code></pre> <ul> <li>Before proceeding to the next step, make sure that the previous command has completed successfully as per the output shown in the screenshot above. </li> <li>On the mongo-0 pod, initialise the Mongo database Replica set. In the terminal run the following command:</li> </ul> <pre><code>cat &lt;&lt; EOF | kubectl exec -it mongo-0 -- mongo\nrs.initiate();\nsleep(2000);\nrs.add(\"mongo-1.mongo:27017\");\nsleep(2000);\nrs.add(\"mongo-2.mongo:27017\");\nsleep(2000);\ncfg = rs.conf();\ncfg.members[0].host = \"mongo-0.mongo:27017\";\nrs.reconfig(cfg, {force: true});\nsleep(5000);\nEOF\n</code></pre> <ul> <li>Confirm that the MongoDB database replica set has been correctly established. In the terminal run the following command:</li> </ul> <pre><code>kubectl exec -it mongo-0 -- mongo --eval \"rs.status()\" | grep \"PRIMARY\\|SECONDARY\"\n</code></pre> <ul> <li>Load the initial voting app data into the MongoDB database.</li> <li>Confirm the voting app data has been loaded correctly. In the terminal run the following command:</li> </ul> <pre><code>kubectl exec -it mongo-0 -- mongo langdb --eval \"db.languages.find().pretty()\"\n</code></pre>"},{"location":"eks/#api-backend-deployment","title":"API (Backend) Deployment","text":"<ul> <li>Create a secret to store the MongoDB connection credentials</li> <li>Generate username and password credentials:</li> </ul> <pre><code>echo -n 'admin' | base64\necho -n 'password' | base64\n</code></pre> <ul> <li>Create a Secret resource within the cluster to hold the base64 encoded credentials.</li> </ul> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mongodb-secret\n  namespace: cloudacademy\ndata:\n  username: YWRtaW4=\n  password: cGFzc3dvcmQ=\nEOF\n</code></pre> <ul> <li>Create the API Deployment resource. </li> <li>Create a new Service resource of LoadBalancer type</li> </ul> <pre><code>kubectl expose deploy api \\\n --name=api \\\n --type=LoadBalancer \\\n --port=80 \\\n --target-port=8080\n</code></pre> <ul> <li>Confirm the API setup within the cluster:<ul> <li>Examine the rollout of the API deployment</li> <li>Examine the pods to confirm that they are up and running</li> <li>Examine the API service details</li> </ul> </li> </ul> <pre><code>{\nkubectl rollout status deployment api\nkubectl get pods -l role=api\nkubectl get svc api\n}\n</code></pre> <ul> <li>Test and confirm that the API route URL <code>/ok</code> endpoint can be called successfully. In the terminal run the command given below. DNS propagation can take up to 2-5 minutes, please be patient while the propagation proceeds - it will eventually complete.</li> </ul> <pre><code>{\nAPI_ELB_PUBLIC_FQDN=$(kubectl get svc api -ojsonpath=\"{.status.loadBalancer.ingress[0].hostname}\")\nuntil nslookup $API_ELB_PUBLIC_FQDN &gt;/dev/null 2&gt;&amp;1; do sleep 2 &amp;&amp; echo waiting for DNS to propagate...; done\ncurl $API_ELB_PUBLIC_FQDN/ok\necho\n}\n</code></pre> <ul> <li>Test and confirm that the API route URL /languages, and /languages/{name} endpoints can be called successfully. In the terminal run any of the following commands:</li> </ul> <pre><code>curl -s $API_ELB_PUBLIC_FQDN/languages | jq .\ncurl -s $API_ELB_PUBLIC_FQDN/languages/go | jq .\ncurl -s $API_ELB_PUBLIC_FQDN/languages/java | jq .\ncurl -s $API_ELB_PUBLIC_FQDN/languages/nodejs | jq .\n</code></pre>"},{"location":"eks/#frontend-deployment","title":"Frontend Deployment","text":"<p>The frontend deployment steps are same as the backend deployment steps - </p> <ul> <li>Retrieve the FQDN of the API LoadBalancer and store it in the <code>$API_PUBLIC_FQDN</code> variable. The value stored in the <code>$API_PUBLIC_FQDN</code> variable is injected into the Frontend container's <code>REACT_APP_APIHOSTPORT</code> environment var - this tells the frontend where to send browser initiated API AJAX calls. In the terminal run the following command:</li> </ul> <pre><code>{\nAPI_ELB_PUBLIC_FQDN=$(kubectl get svc api -ojsonpath=\"{.status.loadBalancer.ingress[0].hostname}\")\necho API_ELB_PUBLIC_FQDN=$API_ELB_PUBLIC_FQDN\n}\n</code></pre> <ul> <li>Create the Frontend Deployment resource</li> <li>Create a new Service resource of LoadBalancer type</li> <li>Confirm the Frontend setup within the cluster:<ul> <li>Examine the rollout of the Frontend deployment</li> <li>Examine the pods to confirm that they are up and running</li> <li>Examine the Frontend service details</li> </ul> </li> </ul> <pre><code>{\nkubectl rollout status deployment frontend\nkubectl get pods -l role=frontend\nkubectl get svc frontend\n}\n</code></pre> <ul> <li>Confirm that the Frontend ELB is ready to recieve HTTP traffic. In the terminal run the following command:</li> </ul> <pre><code>{\nFRONTEND_ELB_PUBLIC_FQDN=$(kubectl get svc frontend -ojsonpath=\"{.status.loadBalancer.ingress[0].hostname}\")\nuntil nslookup $FRONTEND_ELB_PUBLIC_FQDN &gt;/dev/null 2&gt;&amp;1; do sleep 2 &amp;&amp; echo waiting for DNS to propagate...; done\ncurl -I $FRONTEND_ELB_PUBLIC_FQDN\n}\n</code></pre> <ul> <li>Generate the Frontend URL for browsing. In the terminal run the following command:</li> </ul> <pre><code>echo http://$FRONTEND_ELB_PUBLIC_FQDN\n</code></pre> <ul> <li>Query the MongoDB database directly to observe the updated vote data. In the terminal execute the following command:</li> </ul> <pre><code>kubectl exec -it mongo-0 -- mongo langdb --eval \"db.languages.find().pretty()\"\n</code></pre>"},{"location":"eks/#associating-iam-roles-with-kubernetes-service-accounts-in-amazon-eks","title":"Associating IAM Roles with Kubernetes Service Accounts in Amazon EKS","text":"<p>In Kubernetes, a Service Account resource provides an identity for processes that run in a Pod. </p> <p>In Amazon Web Services, an Identity and Access Management (IAM) role performs a similar function. IAM roles provide an identity that permissions can be attached to.</p> <p>Amazon Elastic Kubernetes Service (EKS) allows you to associate a Kubernetes Service Account with an AWS IAM role. Doing this has the following best-practice security benefits:</p> <ul> <li>You can ensure that a Pod only has access to the AWS resources that it requires, adhering to the Principle of Least Privilege</li> <li>Ensuring that a Pod's AWS IAM credentials are separate from other AWS IAM roles, achieving isolation of credentials</li> <li>When using IAM and Service Accounts together, you can use Amazon CloudTrail to audit credential access</li> </ul> <p>Using IAM roles with Kubernetes Service Accounts requires an Open ID Connect (OIDC). EKS can be configured to have a public OIDC discovery endpoint hosted by Amazon for a specific EKS cluster. Using this EKS cluster-specific OIDC provider enables external systems, such as AWS IAM, to accept and verify Kubernetes Service Account tokens. Once an EKS cluster has OIDC configured, it can make use of any of the features of AWS IAM.</p> <p>Now we want to show a demonstration, where you will deploy an application that accesses Amazon S3, you will create a new Service Account and associate it with an existing IAM role, and you will verify that the application can access Amazon S3.</p>"},{"location":"eks/#setting-up-iam-roles","title":"Setting up IAM Roles","text":"<ul> <li>Ensure that you have an up to date <code>kubeconfig</code> file</li> </ul> <pre><code>cd ~\naws eks --region us-west-2 update-kubeconfig --name Cluster-1\n</code></pre> <ul> <li>You can create a seperate namespace and set context for that namespace.</li> </ul> <pre><code>kubectl create namespace iam-oidc\nkubectl config set-context $(kubectl config current-context) --namespace=iam-oidc\n</code></pre> <ul> <li>To view an IAM role that has Amazon S3 access, enter the following:</li> </ul> <pre><code>aws iam get-role --role-name s3-poller-role\n</code></pre> <p>In this case, we will have JSON response showing the <code>AssumeRolePolicyDocument</code>. This allows use of the <code>sts:AssumeRoleWithWebIdentity</code> action by default. The <code>principal</code> for this statement is <code>Federated</code> which allows the use of AWS OIDC Provider as an Amazon Resource Name (ARN). The value of the <code>StringEquals</code> key has a value containing a subset of the <code>Principal</code>'s ARN. This is the identifier for the cluster's OIDC endpoint.</p> <ul> <li>To see the policy attached to the <code>s3-poller-role</code>, enter the following:</li> </ul> <pre><code>aws iam get-role-policy --role-name s3-poller-role --policy-name s3-poller-policy\n</code></pre> <p>This policy allows the <code>s3:List*</code> and <code>s3:PutObject</code> IAM actions. You will associate this pre-created S3 role with a Kubernetes Service Account.</p> <ul> <li>To see this EKS cluster's OIDC endpoint, issue the following in the terminal:</li> </ul> <pre><code>aws eks describe-cluster --name Cluster-1 --region us-west-2 --query cluster.identity\n</code></pre> <p>In our case, OIDC is already configured. To turn it on for an EKS cluster that doesn't have you can use the eksctl tool in the following way:</p> <pre><code>eksctl utils associate-iam-oidc-provider --cluster=&lt;clusterName&gt;\n</code></pre> <ul> <li>To store the bucket in Amazon S3 in a shell variable, issue the following:</li> </ul> <pre><code>BUCKET_NAME=$(aws s3api list-buckets --query \"Buckets[0].Name\" --output text)\n</code></pre> <ul> <li>Create a new deployment manifest and apply it.</li> <li>To update the bucket name in the manifest, use the <code>sed</code> command.</li> <li>To check that a Pod has been created by the Deployment, enter: <code>kubectl get pod</code> or you can watch it using <code>kubectl get pod -w</code></li> <li>To view Pod's logs - </li> </ul> <pre><code>kubectl logs -l app=s3-poller -f\n</code></pre> <p>The logs of all Pods that have an app label with a value of s3-poller will be displayed. The -f flag is short for follow and it means that kubectl will wait for new log entries and print them as they happen.</p> <p>The logs of all Pods that have an app label with a value of s3-poller will be displayed. The container running inside the Pod does not have credentials to access AWS resources. You will configure a Service Account for the Pod that is linked to an IAM role so that the container can access Amazon S3.</p> <ul> <li>To stop following the logs, press Ctrl-C.</li> <li>To list currently existing Service Accounts, issue the following:</li> </ul> <pre><code>kubectl get serviceaccount\n</code></pre> <p>You will see one Service Account named <code>default</code> listed. A default Service Account is automatically created when a new namespace is created. If no Service Account is specified for a Pod, it will use the namespace's default Service Account.</p> <p>Service Accounts have tokens. These are stored in Kubernetes Secret resources. Feel free to issue <code>kubectl get secret</code> to see it listed. Also, feel free to add <code>-o yaml</code> to this command and the instruction's command to see the YAML manifest for these resources.</p> <ul> <li>To create a new Service Account, enter the following: </li> </ul> <pre><code>kubectl create serviceaccount s3-poller\n</code></pre> <ul> <li>To associate the newly created Service Account with an IAM role, enter the following:</li> </ul> <pre><code>kubectl annotate serviceaccount s3-poller \\\n    'eks.amazonaws.com/role-arn'='arn:aws:iam::708048379844:role/s3-poller-role'\n</code></pre> <ul> <li>You can verify the Service Account's configuration by issuing:</li> </ul> <pre><code>kubectl describe serviceaccount s3-poller\n</code></pre> <ul> <li>To modify the Deployment to use your new Service Account, issue the following command:</li> </ul> <pre><code>kubectl patch deployment s3-poller \\\n  -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccountName\":\"s3-poller\"}}}}'\n</code></pre> <ul> <li>To verify that <code>s3-poller</code> Pod can now access Amazon S3, re-issue the logs command from earlier:</li> </ul> <pre><code>kubectl logs -l app=s3-poller\n</code></pre>"},{"location":"eks/#aws-load-balancer-controller-and-ingress-resource","title":"AWS Load Balancer Controller and Ingress Resource","text":"<ul> <li>Before creating the Load Balancer Controller, you need to perform some EKS cluster configuration. The EKS cluster can either be created using GUI or in this case, we will be using <code>eksctl</code> utility. It can be created with the following setting:</li> </ul> <pre><code>eksctl create cluster \\\n--version=1.31 \\\n--name=Cluster-1 \\\n--nodes=1 \\\n--node-type=t2.medium \\\n--ssh-public-key=\"cloudacademylab\" \\\n--region=us-west-2 \\\n--zones=us-west-2a,us-west-2b,us-west-2c \\\n--node-volume-type=gp2 \\\n--node-volume-size=20\n</code></pre> <ul> <li>An OpenID Connect provider needs to be established. This was performed using the following command:</li> </ul> <pre><code>eksctl utils associate-iam-oidc-provider \\\n--region us-west-2 \\\n--cluster Cluster-1 \\\n--approve\n</code></pre> <ul> <li>A new IAM Policy was created, providing various permissions required to provision the underlying infrastructure items (ALBs and/or NLBs) created when Ingress and Service cluster resources are created.</li> </ul> <pre><code>curl -o /tmp/iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json\naws iam create-policy \\\n--policy-name AWSLoadBalancerControllerIAMPolicy \\\n--policy-document file:///tmp/iam_policy.json\n</code></pre> <ul> <li>A new cluster service account bound to the IAM policy has been created. When the AWS Load Balancer controller is later deployed by you, it will be configured to operate with this service account:</li> </ul> <pre><code>eksctl create iamserviceaccount \\\n--cluster Cluster-1 \\\n--namespace kube-system \\\n--name aws-load-balancer-controller \\\n--attach-policy-arn arn:aws:iam::${AWS::AccountId}:policy/AWSLoadBalancerControllerIAMPolicy \\\n--override-existing-serviceaccounts \\\n--approve\n</code></pre>"},{"location":"eks/#deploy-aws-load-balancer-controller","title":"Deploy AWS Load Balancer Controller","text":"<ul> <li>Install the helm utility. Helm will be used to install the AWS Load Balancer Controller chart. In the terminal run the following commands:</li> </ul> <pre><code>{\npushd /tmp\ncurl -o helm-v3.12.1-linux-amd64.tar.gz https://get.helm.sh/helm-v3.9.2-linux-amd64.tar.gz\ntar -xvf helm-v3.12.1-linux-amd64.tar.gz\nsudo mv linux-amd64/helm /usr/local/bin/helm\npopd\nwhich helm\n}\n</code></pre> <ul> <li>Update the local Helm repo</li> </ul> <pre><code>{\nhelm repo add eks https://aws.github.io/eks-charts\nhelm repo update\n}\n</code></pre> <ul> <li>Confirm that a cluster service account exists for the AWS Load Balancer Controller. In the terminal run the following command:</li> </ul> <pre><code>kubectl get sa aws-load-balancer-controller -n kube-system -o yaml\n</code></pre> <ul> <li>Retrieve the VPC ID for the VPC that the EKS cluster worker node is deployed within</li> </ul> <pre><code>{\nVPC_ID=$(aws eks describe-cluster \\\n--name Cluster-1 \\\n--query \"cluster.resourcesVpcConfig.vpcId\" \\\n--output text)\necho $VPC_ID\n}\n</code></pre> <ul> <li>Deploy the Custom Resource Definition (CRD) resources required by the AWS Load Balancer Controller. In the terminal run the following command:</li> </ul> <pre><code>kubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller/crds?ref=master\"\n</code></pre> <ul> <li>Using the helm command, deploy the AWS Load Balancer Controller chart. In the terminal run the following command:</li> </ul> <pre><code>helm install \\\naws-load-balancer-controller \\\neks/aws-load-balancer-controller \\\n-n kube-system \\\n--set clusterName=Cluster-1 \\\n--set serviceAccount.create=false \\\n--set serviceAccount.name=aws-load-balancer-controller \\\n--set image.tag=v2.5.2 \\\n--set region=us-west-2 \\\n--set vpcId=${VPC_ID} \\\n--version=1.5.3\n</code></pre> <ul> <li>Confirm that the AWS Load Balancer Controller has been successfully deployed into the cluster. In the terminal run the following command:</li> </ul> <pre><code>kubectl -n kube-system rollout status deployment aws-load-balancer-controller\n</code></pre> <ul> <li>Examine details of the deployed AWS Load Balancer Controller. In the terminal run the following command:</li> </ul> <pre><code>kubectl describe deployment -n kube-system aws-load-balancer-controller\n</code></pre>"},{"location":"eks/#deploy-and-expose-the-web-app","title":"Deploy and Expose the Web App","text":"<p>In this lab step, you will deploy and set up a sample web app that when requested simply returns a static message on a coloured background. Both the message and the background colour are parameterized, with their configurations specified using ConfigMap resources, mounted to within the pod at launch time. You will launch 2 versions of the same web app, considered V1 (yellow) and V2 (cyan), each with it's own unique message and background colour. Finally, both versions of the web app will be exposed publicly using an Ingress resource - resulting in an ALB being launched behind the scenes. HTTP path based routing will be accomplished by leveraging Annotations specified within the Ingress manifest. As a bonus you will also configure an addtional path route (white) which responds with a static message specified directly within the annotation itself.</p> <p>Choosing an Ingress resource to expose your cluster hosted application to the Internet, will result in the ALBC provisioning an ALB.</p> <ul> <li>Create a dedicated namespace to host both versions of the sample web app</li> </ul> <pre><code>{\nkubectl create ns webapp\nkubectl config set-context $(kubectl config current-context) --namespace=webapp\n}\n</code></pre> <ul> <li>Create a ConfigMap for each version of the sample web app. Each ConfigMap will contain a unique message and background colour. Confirm both ConfigMaps were created successfully. In the terminal run the following command: <code>kubectl get cm webapp-cfg-v1 webapp-cfg-v2</code></li> <li>Create a Deployment for each version of the sample web app. Create both the v1 and v2 deployment resource. </li> </ul> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-v1\n  namespace: webapp\n  labels:\n    role: frontend\n    version: v1\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      role: frontend\n      version: v1\n  template:\n    metadata:\n      labels:\n        role: frontend\n        version: v1\n    spec:\n      containers:\n      - name: webapp\n        image: cloudacademydevops/webappecho:v3\n        imagePullPolicy: IfNotPresent\n        command: [\"/go/bin/webapp\"]\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MESSAGE\n          valueFrom:\n            configMapKeyRef:\n              name: webapp-cfg-v1\n              key: message\n        - name: BACKGROUND_COLOR\n          valueFrom:\n            configMapKeyRef:\n              name: webapp-cfg-v1\n              key: bgcolor\nEOF\n</code></pre> <ul> <li>Confirm both Deployments were rolled out successfully. In the terminal run the following command:</li> </ul> <pre><code>{\nkubectl rollout status deployment frontend-v1\nkubectl rollout status deployment frontend-v2\n}\n</code></pre> <ul> <li>Create a Service for each version of the sample web app</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: frontend-v1\n  namespace: webapp\n  labels:\n    role: frontend\n    version: v1\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    role: frontend\n    version: v1\n  type: NodePort\nEOF\n</code></pre> <ul> <li>Confirm both Services have been created and respective Pod endpoints (IPs) have been registered successfully. In the terminal run the following command:</li> </ul> <pre><code>kubectl get svc,ep\n</code></pre> <ul> <li>Initially expose just the V1 Service to the Internet. Create an Ingress resource for the V1 sample web app. In the terminal run the following command:</li> </ul> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: frontend\n  namespace: webapp\n  annotations:\n    kubernetes.io/ingress.class: alb\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/target-type: ip\nspec:\n  rules:\n    - http:\n        paths:\n          - pathType: Prefix\n            path: /\n            backend:\n              service:\n                name: frontend-v1\n                port:\n                  number: 80\nEOF\n</code></pre> <ul> <li>Confirm that the Ingress resource was created successfully. In the terminal run the following command:</li> </ul> <pre><code>kubectl get ingress frontend\n</code></pre> <ul> <li>Confirm that the ALB and associated DNS records are created for the Ingress resource. In the terminal run the following command:</li> </ul> <pre><code>{\nALB_FQDN=$(kubectl -n webapp get ingress frontend -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\necho ALB_FQDN=$ALB_FQDN\nuntil nslookup $ALB_FQDN &gt;/dev/null 2&gt;&amp;1; do sleep 2 &amp;&amp; echo waiting for DNS to propagate...; done\nuntil curl --silent $ALB_FQDN | grep CloudAcademy &gt;/dev/null 2&gt;&amp;1; do sleep 2 &amp;&amp; echo waiting for ALB to register targets...; done\ncurl -I $ALB_FQDN\necho\necho READY...\necho browse to:\necho \" http://$ALB_FQDN\"\n}\n</code></pre> <ul> <li>Update the Ingress resource to include annotations for routing HTTP traffic. Create a new file named annotations.config and populate it with path routing configuration. In the terminal run the following command:</li> </ul> <pre><code>kubernetes.io/ingress.class: alb\nalb.ingress.kubernetes.io/scheme: internet-facing\nalb.ingress.kubernetes.io/target-type: ip\nalb.ingress.kubernetes.io/actions.forward-tg-svc1: &gt;\n  {\"type\":\"forward\",\"forwardConfig\":{\"targetGroups\":[{\"serviceName\":\"frontend-v1\",\"servicePort\":\"80\"}]}}\nalb.ingress.kubernetes.io/actions.forward-tg-svc2: &gt;\n  {\"type\":\"forward\",\"forwardConfig\":{\"targetGroups\":[{\"serviceName\":\"frontend-v2\",\"servicePort\":\"80\"}]}}\nalb.ingress.kubernetes.io/actions.custom-path1: &gt;\n  {\"type\":\"fixed-response\",\"fixedResponseConfig\":{\"contentType\":\"text/plain\",\"statusCode\":\"200\",\"messageBody\":\"follow the white rabbit...\"}}\nEOF\n</code></pre> <ul> <li>Capture the contents of the <code>annotations.config</code> file in a variable named <code>ANNOTATIONS</code>. In the terminal run the following command:</li> </ul> <pre><code>{\nANNOTATIONS=\"$(cat annotations.config)\"\necho \"$ANNOTATIONS\"\n}\n</code></pre> <ul> <li>Create a new file named ingress.annotations.yaml and populate it with the Ingress manifest configuration, injecting the updated set of path routing annotations. In the terminal run the following command:</li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: frontend\n  namespace: webapp\n  annotations:\n$ANNOTATIONS\nspec:\n  rules:\n    - http:\n        paths:\n          - path: /yellow\n            pathType: Exact\n            backend:\n              service:\n                name: forward-tg-svc1\n                port:\n                  name: use-annotation\n          - path: /cyan\n            pathType: Exact\n            backend:\n              service:\n                name: forward-tg-svc2\n                port:\n                  name: use-annotation\n          - path: /white\n            pathType: Exact\n            backend:\n              service:\n                name: custom-path1\n                port:\n                  name: use-annotation\nEOF\n</code></pre> <ul> <li>Apply the updated Ingress into the cluster - <code>kubectl apply -f ingress.annotations.yaml</code></li> </ul>"},{"location":"elb/","title":"Load Balancers","text":""},{"location":"elb/#elastic-load-balancer-elb","title":"Elastic Load Balancer (ELB)","text":"<p>The main function of an ELB is to help manage and control the flow of inbound requests destined to a group of targets by distributing these requests evenly accross the targeted resource group. These targets could be a group of EC2s, lambda functions, a range of IP addresses or even Containers. </p>"},{"location":"elb/#application-load-balancer","title":"Application Load Balancer","text":"<p>A load balancer serves as the single point of contact for clients. The load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. This increases the availability of your application. You add one or more listeners to your load balancer. </p> <p>You can set port for HTTP or HTTPS requests in your ALB. It operates at the request level. You also have Advanced routing, TLS termination and visibility features targeted at application architectures.</p>"},{"location":"elb/#network-load-balancer","title":"Network Load Balancer","text":"<ul> <li>Ultra-high performance while maintaining very low latencies </li> <li>Operates at the connection level, routing traffic to targets within your VPC</li> <li>Handles millions of requests per second</li> </ul>"},{"location":"elb/#route-based-load-balancing-path-based-routing","title":"Route-Based Load Balancing (Path-Based Routing):","text":"<p>Route-based load balancing (also called path-based routing) distributes traffic to different target groups based on the request URL path.  The load balancer inspects the URL path of the incoming HTTP(S) request. Based on the path, it routes the traffic to a specific target group or service.</p> <p>Example:</p> <ul> <li>Requests to example.com/frontend are routed to the frontend servers.</li> <li>Requests to example.com/api are routed to the API servers.</li> </ul> <p>In AWS, you can configure path-based routing with an Application Load Balancer (ALB). You would define rules in the load balancer's listener based on URL paths (e.g., /images/, /api/), which then forwards the request to different target groups.</p>"},{"location":"elb/#host-based-load-balancing","title":"Host-Based Load Balancing","text":"<p>Host-based load balancing (also called host header-based routing) distributes traffic based on the hostname in the HTTP request. This allows you to direct traffic to different services or applications based on the domain or subdomain.</p> <p>The load balancer inspects the Host header of the incoming HTTP(S) request.</p> <p>Example:</p> <p>A company has multiple domains or subdomains -</p> <ul> <li>www.example.com for the main website.</li> <li>api.example.com for their API.</li> <li>Requests to www.example.com go to the web servers.</li> <li>Requests to api.example.com go to the API servers.</li> </ul>"},{"location":"elb/#classic-load-balancer","title":"Classic Load Balancer","text":"<ul> <li>Used for applications that were build in the existing EC2 Classic Environment</li> <li>Operates at both the connection and request level</li> </ul>"},{"location":"elb/#elb-components","title":"ELB Components","text":"<p>Listeners: The rules that you define for a listener determine how the load balancer routes requests to its registered targets. Each rule consists of a priority, one or more actions, and one or more conditions.</p> <p>Target Groups: Each target group routes requests to one or more registered targets, such as EC2 instances, using the protocol and port number that you specify. You can register a target with multiple target groups. You can configure health checks on a per target group basis. </p> <p>Rules: Rules are associated to each listener that you have configured withing your ELB. They help to define how an incoming request gets routed to which target group.</p> <p>Health Checks: A health check is performed against the resources defined within the target group. These health checks allow the ELB to contact each target using a specific protcol to receive a response.</p> <p>Internet-facing ELB: The nodes of the ELB are accessible via the internt and so have a public DNS name that can be resolved to its Public IP address, in addition to an internal IP address.</p> <p>Internal ELB: An internal ELB only has an internal IP address, this means that it can only serve requests that originate from within your VPC itself.</p> <p>ELB Nodes: For each AZ selected an ELB node will be placed within that AZ.</p>"},{"location":"elb/#ssl-server-certificates","title":"SSL Server Certificates","text":"<p>IF we set HTTPS as our listener, then it will allow us an encrypted communication channnel to be set up between clients initiating the request and your ALB. But to allow ALB to receive encrypted traffic over HTTPS it will need a server certificate and an associated security policy. </p> <p>SSL (Secure Sockets Layer) is a cryptography protocol, much like TLS (Transport Layer Security). Both SSL and TSL are used interchangebly when discussing certificates on your ALB.</p> <p>The server certificate used by ALB is an X.509 certificate, which is a digital ID provisioned by AWS Certificate Manager (ACM). </p>"},{"location":"elb/#network-load-balancer_1","title":"Network Load Balancer","text":"<p>The network load balancer operates at layer 4 of the OSI model enabling you to balance requests purely based upon the TCP protocol. The NLB can process requests in TCP, TLS, and UDP. It is able to process millions of requests per second which makes it ideal for high performance applications.</p> <p>If your application logic requires a static IP address, then NLB will need to be your choice of ELB.</p>"},{"location":"elb/#ec2-auto-scaling","title":"EC2 Auto Scaling","text":"<p>Auto scaling is the mechanism that allows you to increase or decrease your EC2 resources to meet the demand based off of custom defined metrics and thresholds.</p>"},{"location":"elb/#auto-scaling-in-aws","title":"Auto Scaling in AWS","text":"<ul> <li>Amazon EC2 Auto Scaling</li> <li>AWS Auto Scaling - Can automatically scale - Amazon ECS, DynamoDB, Amazon Aurora</li> </ul>"},{"location":"elb/#components-of-ec2-auto-scaling","title":"Components of EC2 Auto Scaling","text":"<ul> <li>Create a Launch Configuration or Launch Template - These define how an Auto Scaling group builds new EC2 instances.</li> <li>Create an Auto Scaling group</li> </ul>"},{"location":"elb/#launch-configuration","title":"Launch Configuration","text":"<ul> <li>Which AMI to use</li> <li>What Instance type</li> <li>If you would like to use Spot Instances</li> <li>If and when Public IP Addresses should be used</li> <li>If any user data is on first boot</li> <li>What storage volume configuration should be used</li> <li>What security groups should be applied</li> </ul>"},{"location":"elb/#launch-template","title":"Launch Template","text":"<p>A launch template is essentially a newer and more advanced version of the launch configuration. Being a template you can build a standard configuration allowing you to simplify how you launch instances for your auto scaling groups. This is the preferred method.</p>"},{"location":"elb/#auto-scaling-groups","title":"Auto Scaling Groups","text":"<p>The auto scaling group defines:</p> <ul> <li>The required capacity and other limitations of the group using scaling policies</li> <li>Where the group should scale resources, such as which AZ</li> </ul>"},{"location":"storage/","title":"Storage Fundamentals for AWS","text":"<p>AWS provides many services for storing data. Data storage can be divided into categories such as:</p> <ul> <li>Block Storage<ul> <li>Data is stored in chunks known as blocks</li> <li>Blocks are stored on a volume and attached to a single instance</li> <li>Provide very low latency</li> </ul> </li> <li>File Storage<ul> <li>Data is stored as seperate files within a series of directories</li> <li>Data is stored on top of a file system</li> <li>Shared access is provided for multiple users</li> </ul> </li> <li>Object Storage<ul> <li>Objects are stored across a flat address space</li> <li>Objects are referenced by a unique key</li> <li>Each object can also have associated metadata to help categorize and identify the object</li> </ul> </li> </ul>"},{"location":"storage/#overview-of-amazon-s3","title":"Overview of Amazon S3","text":"<p>Amazon S3 (Simple Storage Service) is a fully managed Object-based storage service by Amazon, that is - </p> <ul> <li>Highly Available</li> <li>Durable</li> <li>Cost Effective</li> <li>Widely Accessible</li> </ul> <p>It is a object based storage service so each file uploaded does not belong to hierarchial file system but rather file is stored in a flat path and which is referenced by a unique URL. S3 is also a Regional service. </p> <p>To store object in S3, you first need to define and create a bucket. This bucket name must be completetly unique. By default your account can have upto a 100 buckets but this is a soft limit and a request can be made to increase this.</p> <p>Any object uploaded to the bucket are given a unique object key for identification. </p>"},{"location":"storage/#ec2-instance-storage","title":"EC2 Instance Storage","text":"<p>This is also referred to Instance Store Volume. The volumes reside on the same host that provides the EC2 instances itself. The instance storage provides Ephemeral storage for your instance volumes. So it is recommended not to store critical data on these ephemeral volumes. </p> <p>These are the cirumstances where data stored in the EC2 Instance Storage will be lost - </p> <ul> <li>Stop</li> <li>Termniate</li> </ul> <p>However, if the instance was simply rebooted, the data would remain intact</p>"},{"location":"storage/#benefits-of-ec2-instance-storage","title":"Benefits of EC2 Instance Storage","text":"<ul> <li>No additional cost for storage</li> <li>Very high I/O speed</li> <li>Optimized instance families</li> <li>Instance store volumes are ideal as a cache or buffer for rapidly changing data without the need for retention</li> <li>Often used within a load balancing group, where data is replicated and pooled between the fleet</li> </ul>"},{"location":"storage/#amazon-elastic-block-store-ebs","title":"Amazon Elastic Block Store (EBS)","text":"<p>EBS provides storage to EC2 instances with EBS Volume</p> <ul> <li>Provides persistent and durable block level storage</li> <li>EBS volumes offer far more flexiblity with regards to managing the data</li> </ul> <p>EBS volumes are attached to your EC2 instance and are primiarily used for fast changing data that requires a certain amount of Input/Output operations per second (IOPS).</p> <p>EBS also provides the ability to provide point-in-time backup of the storage, called Snapshots. You can manually create a Snapshot of of your storage or automate it using Amazon CloudWatch. The Snapshots themselves are stored in S3 so that they are very durable and reliable.</p> <p>Snapshots are also incremental meaning they will only copy data that has changed since the previous snapshot was taken.</p> <p>Once you have the snapshot of an EBS volume, you can create a new volume from that snapshot. It is also possible to have Snaposhots in different regions. </p> <p>Two types of EBS volumes available -</p> <ul> <li>SSD Backed Storage<ul> <li>Suited for work with smaller blocks</li> <li>As boot volumes for EC</li> </ul> </li> <li>HDD Backed Storage<ul> <li>Suited for workloads that require a higher rate of throughput</li> <li>E.g.: big data and logging information</li> </ul> </li> </ul>"},{"location":"storage/#ebs-security","title":"EBS Security","text":"<p>If you have sensitive information in your storage, then you can just check the box for 'enable EBS security' and EBS will ensure security using Amazon's AES-256 encryption method and AWS KMS (Key Management System). Any snapshot taken from a encrypted volume would also be encrypted. But it is to be noted that this encryption is only available on selected instance types.</p>"},{"location":"storage/#ways-to-create-ebs-volumes","title":"Ways to Create EBS Volumes:","text":"<ul> <li>During the creation of a new instance. You can attach it at the time of launch.</li> <li>From within the EC2 dashboard of the AWS management console. Create it as a standalone volume, ready to be attached to an instance when required.</li> </ul>"},{"location":"storage/#ebs-multi-attach","title":"EBS Multi-Attach","text":"<p>Normally one EBS can only be attached to only one EC2. But with Multi-Attach one EBS can be attached with multiple EC2. But this is heavily dependent on the type of volume and the type of instance. For the time being Multi-Attach is only available in - </p> <ul> <li>Provisioned IOPS SSD (io1)</li> <li>Provisioned IOPS SSD (i02)</li> </ul>"},{"location":"storage/#file-systems","title":"File Systems","text":"<p>It is recommended to use a Clustered File System such as GFS2. This will safely manag multi-instance access to the shared volume.</p>"},{"location":"storage/#nitro-systems","title":"Nitro Systems","text":"<p>EC2 instances based on the Nitro Systems can run EC2 on maximum performance. Nitro instances is a requirement for using Multi-Attach. </p> <p>Nitro: The underlying virtualization platform of the EC2 instance. </p> <p>Bare Metal Insances: The nitro also introduces the concept of Bare Metal Instances. That means you can run EC2 instances without any hypervisor or the customer can use their own hypervisor.</p>"},{"location":"storage/#amazon-elastic-file-system-efs","title":"Amazon Elastic File System (EFS)","text":"<p>It is like your typical directory based file system. This also supports low latency file storage. But unlike EBS, it can provide support to multiple file systems. It also has features of traditional file systems such as - locking files, updating files and renaming files. It also has a hierarchial file structure. This type of storage allows you to store files that are accessible to network resources.</p>"},{"location":"storage/#hybrid-storage-solution-using-aws-storage-gateway","title":"Hybrid Storage Solution using AWS Storage Gateway","text":"<p>AWS Storage Gateway for hybrid storage between on-premises and cloud infrastructure. </p>"},{"location":"storage/#deployment-options","title":"Deployment Options","text":"<ul> <li>On-Premises on a Virtual Machine or AWS Storage Gateway Hardware Appliance</li> <li>Deploy it on a AWS EC2 instance using AWS Storage Gateway Management Service</li> </ul> <p>When deploying the gateway, it will ask for at least 150MB which will work as the local cache. This will act as - </p> <ul> <li>A staging area for data thatthe gateway will upload to AWS</li> <li>A true cache to save data for low-latency access</li> </ul> <p>There are four types of storage gateway - </p> <ul> <li>S3 File Gateway</li> <li>FSx File Gateway</li> <li>Tape Gateway</li> <li>Volume Gateway</li> </ul>"},{"location":"storage/#aws-elastic-disaster-recovery-system-drs","title":"AWS Elastic Disaster Recovery System (DRS)","text":"<p>AWS Elastic DRS is AWS' service for recovering failed applications. It is more efficient and cost-effective than on premises data recovery systems. AWS DRS supports Recovery Point Objectives (RPOs) of seconds and Recovery Time Objectives (RTOs) of minutes by performing ongoing replication of your source servers' disks to EBS volumes in the AWS cloud.</p>"}]}