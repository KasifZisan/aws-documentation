{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Amazon Web Services (AWS)","text":"<p>Fundamental AWS servies - Compute, Storage, Database and Network</p>"},{"location":"#compute","title":"Compute","text":"<p>In the context of AWS (Amazon Web Services), compute refers to the resources and services that enable you to run applications, process data, and execute tasks. Essentially, compute refers to the virtual machines (VMs), containers, or serverless functions that provide the processing power for your workloads.</p> <p>Amazon EC2 - Amazon Elastic Compute Cloud</p>"},{"location":"#storage","title":"Storage","text":"<p>In the context of AWS (Amazon Web Services), storage refers to the services and resources that allow you to store and manage data in the cloud. AWS offers a variety of storage options, each optimized for different types of data, use cases, and performance needs, such as storing files, databases, backups, or archives.</p> <p>Amazon S3 - Amazon Simple Storage Service</p>"},{"location":"ec2/","title":"Amazon Elastic Compute Cloud (EC2)","text":"<p>It allows us to deploy virtual servers within the AWS environment.</p>"},{"location":"ec2/#ec2-components","title":"EC2 Components","text":"<ul> <li>Amazon Machine Images (AMI): Pre configured EC2 instances that allow to quickly launch a new EC2 instance based on the config defined in the AMI. An AMI can help in auto scaling where multiple instances of the AMI can be quickly created.</li> <li>Instance Types: Defines the size of the instance based on different parameters - vCPUs, Architecture, Memory, Storage, Storage type, Network performance</li> <li>Instance purchasing options: On demand instances [can be launched anytime, can be used for as long as needed, flat rate based on instance types, typically used for short-term workloads], Spot Instances [on demand instances that are not taken currently, these are based on supply and demand], Reserved Instances [purchase a discounted on-demand instance for a set period], On-Demand Capacity Reservations [reserve capacity based on attribute]</li> <li>Tenancy: Shared Tenancy [by default, instances run on available hosts with selected resources], Dedicated Tenancy [runs on Dedicated Instances and Dedicated Hosts]</li> <li>Uesr Data: Allows to enter commands that will run during the first boot cycle of that instance. </li> <li>Storage Options: Persistent Storage [EBS Volumes, also can be taken a snapshot of and store in an Amazon S3 Bucket], Ephemeral Storage [Created by EC2 instances using local storage]</li> <li>Security: Security group is an instance level firewall that protects traffic from ingress and egress requests. Restrict communications using source ports and protocols. Key-Pair Login: Private key and Public key system</li> </ul>"},{"location":"ec2/#virtual-private-cloud-vpc","title":"Virtual Private Cloud (VPC)","text":"<p>A VPC (Virtual Private Cloud) is a logically isolated section of a cloud provider\u2019s network.</p> <ul> <li>Isolation: Resources inside a VPC are isolated.</li> <li>Customization: We can customize the IP address range, subnets, routing and security settings like network access control list (ACL) and security groups.</li> <li>Security: Firewalls, Private Subnets and VPNs</li> <li>Network Config: We can configure routing tables, internet gateways and even peering connections.</li> </ul>"},{"location":"ec2/#vpc-endpoint","title":"VPC Endpoint","text":"<p>A VPC Endpoint allows you to privately connect your VPC to support AWS services (like S3, DynamoDB, etc.) without requiring an internet connection, NAT gateway, or VPN connection.</p>"},{"location":"ec2/#cidr-classless-inter-domain-routing","title":"CIDR (Classless Inter-Domain Routing)","text":"<p>It is a method for assigning IP addresses and IP routing that allows more efficient and flexible allocation of IP addresses. It provides a way to define network addresses with variable-length subnet masks, enabling networks of different sizes to be created.</p> <p>CIDR notations are typically as - <code>IP address/prefix length</code></p>"},{"location":"ec2/#subnet","title":"Subnet","text":"<p>A subnet (short for subnetwork) is a logical division of an IP network into smaller, manageable parts. It allows you to break a larger network into smaller segments to improve performance, security, and manageability.</p>"},{"location":"ec2/#subnet-mask","title":"Subnet Mask","text":"<p>A subnet mask is used to separate the network portion of an IP address from the host portion. In CIDR notation, the subnet mask is directly related to the prefix length. For <code>192.168.0.0/24</code>, the subnet mask is written as <code>255.255.255.0</code>.</p>"},{"location":"ec2/#ipv4-and-ipv6-cidr","title":"IPV4 and IPV6 CIDR","text":"<p>IPv4 addresses are 32 bits long, written in four octets (e.g., 192.168.0.1), which gives around 4.3 billion unique addresses.</p> <p>For example - <code>192.168.0.0/24</code>, This represents all IP addresses from <code>192.168.0.0</code> to <code>192.168.0.255</code></p> <p>IPv6 addresses are 128 bits long, written in hexadecimal and separated by colons (e.g., 2001:0db8:85a3::8a2e:0370:7334). IPv6 can support a vastly larger number of addresses\u2014about 340 undecillion addresses.</p>"},{"location":"ec2/#routing-table","title":"Routing Table","text":"<p>In a routing table, the destination and target (sometimes called \"next hop\") columns are key components that determine how network traffic is routed. </p> <ul> <li>Destination Column: The destination column in the routing table specifies the destination network or destination IP address that the packet is trying to reach.</li> <li>Target Column: The target (or \"next hop\") column specifies where the packet should go next to reach the destination.</li> </ul> <p>The CIDR block must not be the same or larger than a destination CIDR range in a route in any of the VPC route tables.</p>"},{"location":"ec2/#internet-gateway-igw","title":"Internet Gateway (IGW)","text":"<p>IGW is a component that helps VPCs connect with the internet or other external networks. It serves as the bridge between the private network (your VPC) and the public Internet.</p> <ul> <li>Bidirectional Traffic:  The Internet Gateway allows instances within the VPC to send and receive traffic from the Internet. </li> <li>Public and Private IP Handling: For the Internet Gateway to work, instances in the VPC need either: Public IP address, Elastic IP address, NAT Gateways</li> </ul>"},{"location":"ec2/#network-address-translation-nat-gateway","title":"Network Address Translation (NAT) Gateway","text":"<p>A NAT gateway enables instances within a private subnet in a VPC to access the internet for outbound traffic. The NAT gateway is a fully managed service provided by AWS, meaning AWS automatically takes care of scaling, fault tolerance, and maintenance. It can scale automatically to accommodate large amounts of traffic.</p>"},{"location":"ec2/#virtual-private-gateway-vgw","title":"Virtual Private Gateway (VGW)","text":"<p>A Virtual Private Gateway (VGW) enables communication between an on-premises network and a cloud-based Virtual Private Cloud (VPC). </p>"},{"location":"eks/","title":"Amazon Elastic Kubernetes Service (EKS)","text":"<p>Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed Kubernetes service that enables you to run Kubernetes seamlessly in both AWS Cloud and on-premises data centers.</p> <p>First you need to get into the AWS platform and then search EC2 and create a Cluster and Node Group. You can then interact with your EKS cluster by SSH-ing into your EKS cluster. Alternatively, you can also set up an EC2 instance to interact with your EKS cluster.</p> <p>In preparation to manage your EKS Kubernetes cluster, you will need to install several Kubernetes management-related tools and utilities. In this lab step, you will install:</p> <ul> <li>kubectl: the Kubernetes command-line utility which is used for communicating with the Kubernetes Cluster API server</li> <li>awscli: used to query and retrieve your Amazon EKS cluster connection details, written into the <code>~/.kube/config</code> file </li> </ul>"},{"location":"eks/#installation-and-setup-steps","title":"Installation and Setup Steps","text":"<ul> <li>Download the <code>kubectl</code> utility, give it executable permissions, and copy it into a directory that is part of the PATH environment variable:</li> </ul> <pre><code>curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.31.0/2024-09-12/bin/linux/amd64/kubectl\nchmod +x ./kubectl\nsudo cp ./kubectl /usr/local/bin\nexport PATH=/usr/local/bin:$PATH\n</code></pre> <ul> <li>Test the kubectl utility, ensuring that it can be called like so:</li> </ul> <pre><code>kubectl version --client=true\n</code></pre> <ul> <li>Download the AWS CLI utility, give it executable permissions, and copy it into a directory that is part of the PATH environment variable:</li> </ul> <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre> <ul> <li>Test the <code>aws</code> utility -</li> </ul> <pre><code>aws --version\n</code></pre> <ul> <li>Use the <code>aws</code> utility, to retrieve EKS Cluster name:</li> </ul> <pre><code>EKS_CLUSTER_NAME=$(aws eks list-clusters --region us-west-2 --query clusters[0] --output text)\necho $EKS_CLUSTER_NAME\n</code></pre> <ul> <li>Use the <code>aws</code> utility to query and retrieve your Amazon EKS cluster connection details, saving them into the ~/.kube/config file. Enter the following command in the terminal:</li> </ul> <pre><code>aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --region us-west-2 \n</code></pre> <ul> <li>View the EKS Cluster connection details. This confirms that the EKS authentication credentials required to authenticate have been correctly copied into the ~/.kube/config file. Enter the following command in the terminal:</li> </ul> <pre><code>cat ~/.kube/config \n</code></pre>"},{"location":"eks/#deploying-a-cloud-native-application","title":"Deploying a Cloud Native Application","text":"<p>The cloud native application has three parts - Frontend, Backend and Database. Both the Frontend and Backend has particular Service and Deployment manifest for them. The Database has StatefulSet, Service, Persistent Volume (PV) and Persistent Volume Claim (PVC).</p> <ul> <li>StatefulSet - used to deploy and launch 3 x pods containing the MongoDB service configured to listen on port 27017</li> <li>Service - headless, will be created to sit in front of the StatefulSet, creating a stable network name for each of the individual pods as well as for the StatefulSet as a whole</li> <li>Persistent Volume (PV) - x3, 1 for each individual pod in the StatefulSet - MongoDB will be configured to persist all data and configuration into a directory mounted to the persistent volume</li> <li>Persistent Volume Claim (PVC) - x3, 1 for each PV, binds a PV to a Pod within the MongoDB StatefulSet</li> </ul>"},{"location":"eks/#deployment-steps","title":"Deployment Steps","text":"<ul> <li>Begin by creating a namespace,  which will be used to contain all of the cluster resources that will eventually make up the sample cloud native application. In the terminal run the following command:</li> </ul> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: cloudacademy\n  labels:\n    name: cloudacademy\nEOF\n</code></pre> <ul> <li>Configure the <code>cloudacademy</code> namespace to be the default. In the terminal run the following command:</li> </ul> <pre><code>kubectl config set-context --current --namespace cloudacademy\n</code></pre>"},{"location":"eks/#mongodb-deployment","title":"MongoDB Deployment","text":"<ul> <li>Deploy the MongoDB 3 x ReplicaSet</li> <li>Display the available EKS storage classes. In the terminal run the following command:</li> </ul> <pre><code>kubectl get storageclass\n</code></pre> <ul> <li>Create a new Mongo StatefulSet name <code>mongo</code>. Also keep in mind to use the <code>storageclass</code> we got before in the <code>storageClassName</code>. <ul> <li>Examine the MongoDB Pods ordered sequence launch. Run a <code>watch</code> on the pods in the current namespace. Wait until all 3 MongoDB pods (<code>mongo-0</code>, <code>mongo-1</code>, <code>mongo-2</code>) have reached and recorded the Running status. <code>kubectl get pods --watch</code></li> <li>View the pods with the db label in the current namespace. In the terminal run the following command: <code>kubectl get pods -l role=db</code></li> <li>Display the MongoDB Pods, Persistent Volumes and Persistent Volume Claims. <code>kubectl get pod,pv,pvc</code></li> </ul> </li> <li>Create a new Headless Service for Mongo. Examine the newly created <code>mongo</code> Headless Service. <code>kubectl get svc</code></li> <li>Create a temporary network utils pod. Enter into a bash session within it. <code>kubectl run --rm utils -it --image praqma/network-multitool -- bash</code>. Creating a temporary utils pod in the current namespace - ensures that the <code>nslookup</code> queries to be performed next, are done so in the same networking space in which the MongoDB deployment has taken place in.<ul> <li>Within the new utils pod shell, execute the following DNS queries: <code>for i in {0..2}; do nslookup mongo-$i.mongo; done</code>. This confirms that the DNS records have been created successfully and can be resolved within the cluster, 1 per MongoDB pod that exists behind the Headless Service - earlier created. Exit the utils container: <code>exit</code></li> </ul> </li> <li>Confirm that the <code>mongo</code> shell can now also resolve each of the 3 Mongo headless Service assigned DNS names. In the terminal run the following command:</li> </ul> <pre><code>for i in {0..2}; do kubectl exec -it mongo-0 -- mongo mongo-$i.mongo --eval \"print('mongo-$i.mongo SUCCEEDED\\n')\"; done\n</code></pre> <ul> <li>Before proceeding to the next step, make sure that the previous command has completed successfully as per the output shown in the screenshot above. </li> <li>On the mongo-0 pod, initialise the Mongo database Replica set. In the terminal run the following command:</li> </ul> <pre><code>cat &lt;&lt; EOF | kubectl exec -it mongo-0 -- mongo\nrs.initiate();\nsleep(2000);\nrs.add(\"mongo-1.mongo:27017\");\nsleep(2000);\nrs.add(\"mongo-2.mongo:27017\");\nsleep(2000);\ncfg = rs.conf();\ncfg.members[0].host = \"mongo-0.mongo:27017\";\nrs.reconfig(cfg, {force: true});\nsleep(5000);\nEOF\n</code></pre> <ul> <li>Confirm that the MongoDB database replica set has been correctly established. In the terminal run the following command:</li> </ul> <pre><code>kubectl exec -it mongo-0 -- mongo --eval \"rs.status()\" | grep \"PRIMARY\\|SECONDARY\"\n</code></pre> <ul> <li>Load the initial voting app data into the MongoDB database.</li> <li>Confirm the voting app data has been loaded correctly. In the terminal run the following command:</li> </ul> <pre><code>kubectl exec -it mongo-0 -- mongo langdb --eval \"db.languages.find().pretty()\"\n</code></pre>"},{"location":"eks/#api-backend-deployment","title":"API (Backend) Deployment","text":"<ul> <li>Create a secret to store the MongoDB connection credentials</li> <li>Generate username and password credentials:</li> </ul> <pre><code>echo -n 'admin' | base64\necho -n 'password' | base64\n</code></pre> <ul> <li>Create a Secret resource within the cluster to hold the base64 encoded credentials.</li> </ul> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mongodb-secret\n  namespace: cloudacademy\ndata:\n  username: YWRtaW4=\n  password: cGFzc3dvcmQ=\nEOF\n</code></pre> <ul> <li>Create the API Deployment resource. </li> <li>Create a new Service resource of LoadBalancer type</li> </ul> <pre><code>kubectl expose deploy api \\\n --name=api \\\n --type=LoadBalancer \\\n --port=80 \\\n --target-port=8080\n</code></pre> <ul> <li>Confirm the API setup within the cluster:<ul> <li>Examine the rollout of the API deployment</li> <li>Examine the pods to confirm that they are up and running</li> <li>Examine the API service details</li> </ul> </li> </ul> <pre><code>{\nkubectl rollout status deployment api\nkubectl get pods -l role=api\nkubectl get svc api\n}\n</code></pre> <ul> <li>Test and confirm that the API route URL <code>/ok</code> endpoint can be called successfully. In the terminal run the command given below. DNS propagation can take up to 2-5 minutes, please be patient while the propagation proceeds - it will eventually complete.</li> </ul> <pre><code>{\nAPI_ELB_PUBLIC_FQDN=$(kubectl get svc api -ojsonpath=\"{.status.loadBalancer.ingress[0].hostname}\")\nuntil nslookup $API_ELB_PUBLIC_FQDN &gt;/dev/null 2&gt;&amp;1; do sleep 2 &amp;&amp; echo waiting for DNS to propagate...; done\ncurl $API_ELB_PUBLIC_FQDN/ok\necho\n}\n</code></pre> <ul> <li>Test and confirm that the API route URL /languages, and /languages/{name} endpoints can be called successfully. In the terminal run any of the following commands:</li> </ul> <pre><code>curl -s $API_ELB_PUBLIC_FQDN/languages | jq .\ncurl -s $API_ELB_PUBLIC_FQDN/languages/go | jq .\ncurl -s $API_ELB_PUBLIC_FQDN/languages/java | jq .\ncurl -s $API_ELB_PUBLIC_FQDN/languages/nodejs | jq .\n</code></pre>"},{"location":"elb/","title":"Load Balancers","text":""},{"location":"elb/#elastic-load-balancer-elb","title":"Elastic Load Balancer (ELB)","text":"<p>The main function of an ELB is to help manage and control the flow of inbound requests destined to a group of targets by distributing these requests evenly accross the targeted resource group. These targets could be a group of EC2s, lambda functions, a range of IP addresses or even Containers. </p>"},{"location":"elb/#application-load-balancer","title":"Application Load Balancer","text":"<p>A load balancer serves as the single point of contact for clients. The load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. This increases the availability of your application. You add one or more listeners to your load balancer. </p> <p>You can set port for HTTP or HTTPS requests in your ALB. It operates at the request level. You also have Advanced routing, TLS termination and visibility features targeted at application architectures.</p>"},{"location":"elb/#network-load-balancer","title":"Network Load Balancer","text":"<ul> <li>Ultra-high performance while maintaining very low latencies </li> <li>Operates at the connection level, routing traffic to targets within your VPC</li> <li>Handles millions of requests per second</li> </ul>"},{"location":"elb/#route-based-load-balancing-path-based-routing","title":"Route-Based Load Balancing (Path-Based Routing):","text":"<p>Route-based load balancing (also called path-based routing) distributes traffic to different target groups based on the request URL path.  The load balancer inspects the URL path of the incoming HTTP(S) request. Based on the path, it routes the traffic to a specific target group or service.</p> <p>Example:</p> <ul> <li>Requests to example.com/frontend are routed to the frontend servers.</li> <li>Requests to example.com/api are routed to the API servers.</li> </ul> <p>In AWS, you can configure path-based routing with an Application Load Balancer (ALB). You would define rules in the load balancer's listener based on URL paths (e.g., /images/, /api/), which then forwards the request to different target groups.</p>"},{"location":"elb/#host-based-load-balancing","title":"Host-Based Load Balancing","text":"<p>Host-based load balancing (also called host header-based routing) distributes traffic based on the hostname in the HTTP request. This allows you to direct traffic to different services or applications based on the domain or subdomain.</p> <p>The load balancer inspects the Host header of the incoming HTTP(S) request.</p> <p>Example:</p> <p>A company has multiple domains or subdomains -</p> <ul> <li>www.example.com for the main website.</li> <li>api.example.com for their API.</li> <li>Requests to www.example.com go to the web servers.</li> <li>Requests to api.example.com go to the API servers.</li> </ul>"},{"location":"elb/#classic-load-balancer","title":"Classic Load Balancer","text":"<ul> <li>Used for applications that were build in the existing EC2 Classic Environment</li> <li>Operates at both the connection and request level</li> </ul>"},{"location":"elb/#elb-components","title":"ELB Components","text":"<p>Listeners: The rules that you define for a listener determine how the load balancer routes requests to its registered targets. Each rule consists of a priority, one or more actions, and one or more conditions.</p> <p>Target Groups: Each target group routes requests to one or more registered targets, such as EC2 instances, using the protocol and port number that you specify. You can register a target with multiple target groups. You can configure health checks on a per target group basis. </p> <p>Rules: Rules are associated to each listener that you have configured withing your ELB. They help to define how an incoming request gets routed to which target group.</p> <p>Health Checks: A health check is performed against the resources defined within the target group. These health checks allow the ELB to contact each target using a specific protcol to receive a response.</p> <p>Internet-facing ELB: The nodes of the ELB are accessible via the internt and so have a public DNS name that can be resolved to its Public IP address, in addition to an internal IP address.</p> <p>Internal ELB: An internal ELB only has an internal IP address, this means that it can only serve requests that originate from within your VPC itself.</p> <p>ELB Nodes: For each AZ selected an ELB node will be placed within that AZ.</p>"},{"location":"elb/#ssl-server-certificates","title":"SSL Server Certificates","text":"<p>IF we set HTTPS as our listener, then it will allow us an encrypted communication channnel to be set up between clients initiating the request and your ALB. But to allow ALB to receive encrypted traffic over HTTPS it will need a server certificate and an associated security policy. </p> <p>SSL (Secure Sockets Layer) is a cryptography protocol, much like TLS (Transport Layer Security). Both SSL and TSL are used interchangebly when discussing certificates on your ALB.</p> <p>The server certificate used by ALB is an X.509 certificate, which is a digital ID provisioned by AWS Certificate Manager (ACM). </p>"},{"location":"elb/#network-load-balancer_1","title":"Network Load Balancer","text":"<p>The network load balancer operates at layer 4 of the OSI model enabling you to balance requests purely based upon the TCP protocol. The NLB can process requests in TCP, TLS, and UDP. It is able to process millions of requests per second which makes it ideal for high performance applications.</p> <p>If your application logic requires a static IP address, then NLB will need to be your choice of ELB.</p>"},{"location":"elb/#ec2-auto-scaling","title":"EC2 Auto Scaling","text":"<p>Auto scaling is the mechanism that allows you to increase or decrease your EC2 resources to meet the demand based off of custom defined metrics and thresholds.</p>"},{"location":"elb/#auto-scaling-in-aws","title":"Auto Scaling in AWS","text":"<ul> <li>Amazon EC2 Auto Scaling</li> <li>AWS Auto Scaling - Can automatically scale - Amazon ECS, DynamoDB, Amazon Aurora</li> </ul>"},{"location":"elb/#components-of-ec2-auto-scaling","title":"Components of EC2 Auto Scaling","text":"<ul> <li>Create a Launch Configuration or Launch Template - These define how an Auto Scaling group builds new EC2 instances.</li> <li>Create an Auto Scaling group</li> </ul>"},{"location":"elb/#launch-configuration","title":"Launch Configuration","text":"<ul> <li>Which AMI to use</li> <li>What Instance type</li> <li>If you would like to use Spot Instances</li> <li>If and when Public IP Addresses should be used</li> <li>If any user data is on first boot</li> <li>What storage volume configuration should be used</li> <li>What security groups should be applied</li> </ul>"},{"location":"elb/#launch-template","title":"Launch Template","text":"<p>A launch template is essentially a newer and more advanced version of the launch configuration. Being a template you can build a standard configuration allowing you to simplify how you launch instances for your auto scaling groups. This is the preferred method.</p>"},{"location":"elb/#auto-scaling-groups","title":"Auto Scaling Groups","text":"<p>The auto scaling group defines:</p> <ul> <li>The required capacity and other limitations of the group using scaling policies</li> <li>Where the group should scale resources, such as which AZ</li> </ul>"},{"location":"storage/","title":"Storage Fundamentals for AWS","text":"<p>AWS provides many services for storing data. Data storage can be divided into categories such as:</p> <ul> <li>Block Storage<ul> <li>Data is stored in chunks known as blocks</li> <li>Blocks are stored on a volume and attached to a single instance</li> <li>Provide very low latency</li> </ul> </li> <li>File Storage<ul> <li>Data is stored as seperate files within a series of directories</li> <li>Data is stored on top of a file system</li> <li>Shared access is provided for multiple users</li> </ul> </li> <li>Object Storage<ul> <li>Objects are stored across a flat address space</li> <li>Objects are referenced by a unique key</li> <li>Each object can also have associated metadata to help categorize and identify the object</li> </ul> </li> </ul>"},{"location":"storage/#overview-of-amazon-s3","title":"Overview of Amazon S3","text":"<p>Amazon S3 (Simple Storage Service) is a fully managed Object-based storage service by Amazon, that is - </p> <ul> <li>Highly Available</li> <li>Durable</li> <li>Cost Effective</li> <li>Widely Accessible</li> </ul> <p>It is a object based storage service so each file uploaded does not belong to hierarchial file system but rather file is stored in a flat path and which is referenced by a unique URL. S3 is also a Regional service. </p> <p>To store object in S3, you first need to define and create a bucket. This bucket name must be completetly unique. By default your account can have upto a 100 buckets but this is a soft limit and a request can be made to increase this.</p> <p>Any object uploaded to the bucket are given a unique object key for identification. </p>"},{"location":"storage/#ec2-instance-storage","title":"EC2 Instance Storage","text":"<p>This is also referred to Instance Store Volume. The volumes reside on the same host that provides the EC2 instances itself. The instance storage provides Ephemeral storage for your instance volumes. So it is recommended not to store critical data on these ephemeral volumes. </p> <p>These are the cirumstances where data stored in the EC2 Instance Storage will be lost - </p> <ul> <li>Stop</li> <li>Termniate</li> </ul> <p>However, if the instance was simply rebooted, the data would remain intact</p>"},{"location":"storage/#benefits-of-ec2-instance-storage","title":"Benefits of EC2 Instance Storage","text":"<ul> <li>No additional cost for storage</li> <li>Very high I/O speed</li> <li>Optimized instance families</li> <li>Instance store volumes are ideal as a cache or buffer for rapidly changing data without the need for retention</li> <li>Often used within a load balancing group, where data is replicated and pooled between the fleet</li> </ul>"},{"location":"storage/#amazon-elastic-block-store-ebs","title":"Amazon Elastic Block Store (EBS)","text":"<p>EBS provides storage to EC2 instances with EBS Volume</p> <ul> <li>Provides persistent and durable block level storage</li> <li>EBS volumes offer far more flexiblity with regards to managing the data</li> </ul> <p>EBS volumes are attached to your EC2 instance and are primiarily used for fast changing data that requires a certain amount of Input/Output operations per second (IOPS).</p> <p>EBS also provides the ability to provide point-in-time backup of the storage, called Snapshots. You can manually create a Snapshot of of your storage or automate it using Amazon CloudWatch. The Snapshots themselves are stored in S3 so that they are very durable and reliable.</p> <p>Snapshots are also incremental meaning they will only copy data that has changed since the previous snapshot was taken.</p> <p>Once you have the snapshot of an EBS volume, you can create a new volume from that snapshot. It is also possible to have Snaposhots in different regions. </p> <p>Two types of EBS volumes available -</p> <ul> <li>SSD Backed Storage<ul> <li>Suited for work with smaller blocks</li> <li>As boot volumes for EC</li> </ul> </li> <li>HDD Backed Storage<ul> <li>Suited for workloads that require a higher rate of throughput</li> <li>E.g.: big data and logging information</li> </ul> </li> </ul>"},{"location":"storage/#ebs-security","title":"EBS Security","text":"<p>If you have sensitive information in your storage, then you can just check the box for 'enable EBS security' and EBS will ensure security using Amazon's AES-256 encryption method and AWS KMS (Key Management System). Any snapshot taken from a encrypted volume would also be encrypted. But it is to be noted that this encryption is only available on selected instance types.</p>"},{"location":"storage/#ways-to-create-ebs-volumes","title":"Ways to Create EBS Volumes:","text":"<ul> <li>During the creation of a new instance. You can attach it at the time of launch.</li> <li>From within the EC2 dashboard of the AWS management console. Create it as a standalone volume, ready to be attached to an instance when required.</li> </ul>"},{"location":"storage/#ebs-multi-attach","title":"EBS Multi-Attach","text":"<p>Normally one EBS can only be attached to only one EC2. But with Multi-Attach one EBS can be attached with multiple EC2. But this is heavily dependent on the type of volume and the type of instance. For the time being Multi-Attach is only available in - </p> <ul> <li>Provisioned IOPS SSD (io1)</li> <li>Provisioned IOPS SSD (i02)</li> </ul>"},{"location":"storage/#file-systems","title":"File Systems","text":"<p>It is recommended to use a Clustered File System such as GFS2. This will safely manag multi-instance access to the shared volume.</p>"},{"location":"storage/#nitro-systems","title":"Nitro Systems","text":"<p>EC2 instances based on the Nitro Systems can run EC2 on maximum performance. Nitro instances is a requirement for using Multi-Attach. </p> <p>Nitro: The underlying virtualization platform of the EC2 instance. </p> <p>Bare Metal Insances: The nitro also introduces the concept of Bare Metal Instances. That means you can run EC2 instances without any hypervisor or the customer can use their own hypervisor.</p>"},{"location":"storage/#amazon-elastic-file-system-efs","title":"Amazon Elastic File System (EFS)","text":"<p>It is like your typical directory based file system. This also supports low latency file storage. But unlike EBS, it can provide support to multiple file systems. It also has features of traditional file systems such as - locking files, updating files and renaming files. It also has a hierarchial file structure. This type of storage allows you to store files that are accessible to network resources.</p>"},{"location":"storage/#hybrid-storage-solution-using-aws-storage-gateway","title":"Hybrid Storage Solution using AWS Storage Gateway","text":"<p>AWS Storage Gateway for hybrid storage between on-premises and cloud infrastructure. </p>"},{"location":"storage/#deployment-options","title":"Deployment Options","text":"<ul> <li>On-Premises on a Virtual Machine or AWS Storage Gateway Hardware Appliance</li> <li>Deploy it on a AWS EC2 instance using AWS Storage Gateway Management Service</li> </ul> <p>When deploying the gateway, it will ask for at least 150MB which will work as the local cache. This will act as - </p> <ul> <li>A staging area for data thatthe gateway will upload to AWS</li> <li>A true cache to save data for low-latency access</li> </ul> <p>There are four types of storage gateway - </p> <ul> <li>S3 File Gateway</li> <li>FSx File Gateway</li> <li>Tape Gateway</li> <li>Volume Gateway</li> </ul>"},{"location":"storage/#aws-elastic-disaster-recovery-system-drs","title":"AWS Elastic Disaster Recovery System (DRS)","text":"<p>AWS Elastic DRS is AWS' service for recovering failed applications. It is more efficient and cost-effective than on premises data recovery systems. AWS DRS supports Recovery Point Objectives (RPOs) of seconds and Recovery Time Objectives (RTOs) of minutes by performing ongoing replication of your source servers' disks to EBS volumes in the AWS cloud.</p>"}]}